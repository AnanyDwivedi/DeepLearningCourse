import numpy,math
import keras,csv,os
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.cross_validation import cross_val_score
from sklearn.cross_validation import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.cross_validation import train_test_split
from sklearn.utils import check_array
from keras.utils.visualize_util import plot
from keras.callbacks import LearningRateScheduler
from keras.callbacks import History

from keras.layers import Activation
from keras.utils.np_utils import to_categorical
#from sklearn.metrics import accuracy_score
from keras import metrics
from keras import backend as K
from sklearn.metrics import f1_score
from sklearn.model_selection import StratifiedShuffleSplit
from keras.models import load_model
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt

def normalizedata(X):
	for i in range(X.shape[1]):
#		print (np.max(np.abs(X[:,i])))
		X[:,i] /= (numpy.max(numpy.abs(X[:,i])))
#	print "\n\n"
	return X

def predictUD(yp,cutof):
	print cutof.shape
	print yp.shape
	print cutof
	p = numpy.zeros((yp.shape[0],1))
	pUO = numpy.zeros((yp.shape[0],1))
	for i in range(yp.shape[0]):
		if yp[i,1] > cutof:
			p[i] = 1
		else:
			p[i] = 0 

	for i in range(yp.shape[0]):
		if yp[i,1] >= 0.5:
			pUO[i] = 1
		else:
			pUO[i] = 0 
	return p,pUO



def Find_Optimal_Cutoff(target, predicted):
	""" Find the optimal probability cutoff point for a classification model related to event rate
	Parameters
	----------
	target : Matrix with dependent or target data, where rows are observations
	
	predicted : Matrix with predicted data, where rows are observations
	
	Returns
	-------     
	list type, with optimal cutoff value
	
	"""
	fpr, tpr, threshold = roc_curve(target, predicted)
	print fpr.shape,tpr.shape,threshold.shape
	print threshold
	i = numpy.arange(len(tpr)) 
	roc = pd.DataFrame({'tf' : pd.Series(tpr-(1- fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})
	roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]
	
	return list(roc_t['threshold']) 
	

# Plot data
def generate_results(y_test, y_score):
	fpr, tpr, _ = roc_curve(y_test, y_score)
	roc_auc = auc(fpr, tpr)
	plt.figure()
	plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
	plt.plot([0, 1], [0, 1], 'k--')
	plt.xlim([0.0, 1.05])
	plt.ylim([0.0, 1.05])
	plt.xlabel('False Positive Rate')
	plt.ylabel('True Positive Rate')
	plt.title('Receiver operating characteristic curve')
#    plt.show()
	plt.savefig('temp.png')
	print('AUC: %f' % roc_auc)

def LoadData():
	X_train = numpy.load('../../../Data/Year1_trainData.npy')
	y_train = numpy.load('../../../Data/Year1_trainLabels.npy')
	X_test = numpy.load('../../../Data/Year1_testData.npy')
	y_test = numpy.load('../../../Data/Year1_testLabels.npy')
	return X_train, y_train, X_test, y_test

def to_categoricalUD(y):
	yc = numpy.zeros((y.shape[0],2))
	j = 0
	for i in range(y.shape[0]):
		if y[i] == 0:
			yc[i,0] = 1
			j += 1
		elif y[i] == 1:
			yc[i,1] = 1
			j += 1
		else:
			yc[i,2] = 1
			j += 1

	return yc

def misclass(t,p):
	z=0
	o=0
#	t = numpy.argmax(tru,1)
#	p = numpy.argmax(pred,1)
	
	Zc = 0
	Oc = 0
	for i in range(t.shape[0]):	
		if t[i] == 0:
			Zc+=1
		else:
			Oc+=1
		
		if t[i]!=p[i]:
			if t[i] == 0:
				z+=1
			else:
				o+=1

	print "\n Z = %d O = %d Zc =%d Oc = %d"%(z,o,Zc,Oc)

def data_split(X,y,n_splits=3, test_size=0.2, random_state=0):
	sss = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=0)
	sss.get_n_splits(X, y)
	for train_index, test_index in sss.split(X, y):
		X_train, X_test = X[train_index], X[test_index]
		y_train, y_test = y[train_index], y[test_index]

	return X_train, X_test, y_train, y_test

def f1sc(y_true, y_pred):
	return f1_score(y_true, y_pred, average='weighted')  

def acc(t,l):
	tru = numpy.argmax(t,1)
	pred = numpy.argmax(l,1)
	acc = numpy.mean(tru == pred)
	return acc

# learning rate schedule
def step_decay(epoch):
        initial_lrate = 0.0001
        drop = 0.5
        epochs_drop = 100.0
        lrate = initial_lrate * math.pow(drop,((1+epoch)/epochs_drop))
        return lrate

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)

# load dataset
X_train, y_train, X_test, y_test = LoadData()
X_train = normalizedata(X_train)
X_test = normalizedata(X_test)
#X = numpy.load('../../../Data/DataYear1.npy')
#Y = numpy.load('../../../Data/DataYear1Labels.npy')

print "DataRead"

# Data Split
#X_train1, X_test, y_train1, y_test = data_split(X, Y, test_size=0.2, random_state=seed)
#X_train, X_val, y_train, y_val = data_split(X_train1, y_train1, test_size=0.1, random_state=seed)
#y_train = to_categoricalUD(y_train)
#y_val = to_categoricalUD(y_val)
#y_test = to_categoricalUD(y_test)

#for i in range(X.shape[1]):
#        X_train[:,i]=numpy.subtract(X_train[:,i],X_train[:,i].mean())
#        X_val[:,i]=numpy.subtract(X_val[:,i],X_val[:,i].mean())
#        X_test[:,i]=numpy.subtract(X_test[:,i],X_test[:,i].mean())

print y_train.shape
print X_train.shape

# create model
model = Sequential()
prelu=keras.layers.advanced_activations.PReLU()
#1336
model.add(Dense(1, input_dim=X_train.shape[1], init='uniform',name='h1'))#, activation='relu'))
model.add(prelu)

model.add(Dense(2, init='uniform',activation='softmax',name='out'))
#model.add(Activation('softmax'))

# Compile model
current_dir = os.path.dirname(os.path.realpath(__file__))
model_path = os.path.join(current_dir, "FullyConnectedNetworkPrelu.png")
plot(model, to_file=model_path, show_shapes=True)
adam=keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
model.compile(loss='binary_crossentropy', optimizer=adam,metrics=['categorical_accuracy'])

# learning schedule callback
chk_point = keras.callbacks.ModelCheckpoint('chk_point_log.hdf5', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto')
history=History()
lrate = LearningRateScheduler(step_decay)
callbacks_list = [lrate,history,chk_point]

#model Fitting
print "Training..."
model.fit(X_train, y_train,validation_data=(X_test,y_test),nb_epoch=150, batch_size=512, callbacks=callbacks_list, verbose=1)

#Model prediction
predicted=model.predict_proba(X_test,batch_size=25)
pred=model.predict_classes(X_test,batch_size=25)

yt = numpy.argmax(y_test,1)
cutof = numpy.array(Find_Optimal_Cutoff(yt, predicted[:,1]))
print "     ", cutof
yp,ypUo = predictUD(predicted,cutof)
#print "Prediction F1 Score: ", f1sc(y_test,y_pred)
#yp = numpy.argmax(y_pred,1)

print "ROC_AUC_OPT: ", roc_auc_score(yt,yp)
print "ROC_AUC: ", roc_auc_score(yt, predicted[:,1])
print "ROC_AUC_Pred: ", roc_auc_score(yt, pred)
misclass(yt,yp)
misclass(yt,ypUo)
misclass(yt,pred)
#generate_results(yt, ypUo)

numpy.save("Prediction.npy",predicted)
numpy.save("Xtest.npy",X_test)
model.save('N1_student.h5')

